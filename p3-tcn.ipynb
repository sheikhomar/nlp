{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper: An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the deep learning community, there is a general tendency to focus on recurrent and recursive neural networks for sequence models. However, recent papers show that that CNN-based architectures perform as well as RNNs for sequence modelling. The authors introduce the Temporal Convolutional Networks (TCN) as a family of CNN-based architectures that use 1D dilated causal convolutions and residual blocks for deeper networks. They show that TCP outperforms recurrent architectures such as LSTMs and GRUs for many sequence modelling tasks. They also show that TCNs exhibit longer memory than RNNs with the same capacity. They state that TCN is a relatively new architecture with the potential to advance its performance as more researchers look into it. Because of the simplicity and performance of TCN, the authors argue that the architecture may be a more appropriate starting point when applying deep learning methods to sequences. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Modeling Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence modelling task is given as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#E0FFFF;border:solid 1px #ddd; padding:10px\">\n",
    "Given an input sequence $[x_0, x_1, \\cdots, x_T]$, we want to predict some corresponding output sequence $[y_0, y_1, \\cdots, y_T]$ at each time step $t$.\n",
    "<br><br>\n",
    "\n",
    "The key constraint is that the prediction of output value $y_t$ at time $t$ must only use the input values observed until time $t$ i.e., we can only predict $y_t$ using the \"past\" input values $x_0, x_1, \\cdots, x_t$ and not \"future\" values $x_{t+1}, x_{t+2}, \\cdots, x_T$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is known as the __causal__ sequence modelling. \n",
    " - It captures many settings such as autoregressive prediction (where we try to predict some signal given its past) by setting the target output to be simply the input shifted by one time step.\n",
    " - It does not capture machine translation or sequence-to-sequence prediction in general because these use the entire input sequence (including \"future\" states) to predict each output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks\n",
    "- Often used for dedicated sequence models\n",
    "- Used for language modeling and machine translation\n",
    "- Maintain a vector of hidden activations that are propagated through time\n",
    "- The hidden state can act as a representation of everything that has been seen so far in the sequence\n",
    "- Basic RNN architectures are notoriously difficult to train. Instead elaborate architectures like LSTM and GRU is used\n",
    "- Architectures that are much better than the LSTM are not trivial to find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combination of RNN and CNN to extrac the desirable aspects of both types of architectures\n",
    "- Convolutional LSTM: Replaces the fully-connected layers in an LSTM with convolutional layers to allow for additional structure in the recurrent layers\n",
    "- Quasi-RNN: interleaves convolutional layers with simple recurrent layers\n",
    "- Dilated RNN: adds dilations to recurrent architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Temporal Convolutional Networks (TCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TCN is used as a simple descriptive term for a family of architectures. It is supposed to be a ConvNet design that distill the best practices for modelling sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TCN consists of two main building blocks:\n",
    "- 1D Fully-Convolutional Network (FCN) because we want the network to an output of the same length as the input\n",
    "- Causal convolutions to only allow predictions to be based on past values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TCN = 1D FCN + causal convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network that consists of the above-mentioned two building blocks is essentially a Time Delay Neural Network invented in 1989."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve it, TCN uses following modern techniques:\n",
    "- __Dilated Convolutions__ allows the network to have an exponentially large receptive field giving the TCN very long effective memory.\n",
    "- __Residual Blocks__ allows the network to be very deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Convolutional Network (FCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each hidden layer in a 1D fully-convolutional network (FCN) architecture has the same length as the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p3/1d-fcn-no-connections.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Causal convolution means that the output of time $t$ is convolved only with elements from time $t$ and earlier\n",
    "- Causal convolution can be implemented by shifting the output of a normal convolution by a few timesteps\n",
    "\n",
    "```python\n",
    "def CausalConv1d(in_channels, out_channels, kernel_size, **kwargs):\n",
    "    pad = (kernel_size - 1)\n",
    "    return Conv1d(in_channels, out_channels, kernel_size, padding=pad, **kwargs)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p3/causal-convolutional-layers.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "Source: https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_conv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dialated Causal Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dilated convolutions allows the network to have an exponentially large receptive field:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows an architecture with dilated causal convolutions with dilation factors $d = 1, 2, 4, 8$ and filter size $k = 2$. The receptive field is able to cover all values from the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p3/dialated-causal-convolutional-layers.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dialation factor $d$ is increased exponentially with the depth of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A residual block consists of two branches of computation:\n",
    "- One branch performs a series of transformations to the input of the block. \n",
    "- The second branch is an identity mapping i.e., meaning it does not perform any transformation. \n",
    "\n",
    "The output of the block is the addition of the two branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $\\mathbf{z}^{(i-1)}$ is the input to the block, $\\mathcal{F}$ describes the set of transformations performed by the residual block and $g$ is the activation function. The output of the block is then:\n",
    "\n",
    "$$\n",
    "o = g \\left[ \\mathbf{z}^{(i-1)} + \\mathcal{F} \\left(\\mathbf{z}^{(i-1)} \\right) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Why use residual blocks?__\n",
    "- allows layers to learn modifications to the identity mapping rather than the entire transformation\n",
    "- it has repeatedly been shown to benefit very deep networks\n",
    "  - We want deep network because TCN's receptive field depends on the network depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure illustrates how the residual block can look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p3/residual-block-illustration.png\" width=\"300\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generic TCN model uses residual module instead of a convolutional layer:\n",
    "- two layers of dilated causal convolution \n",
    "- non-linearity with ReLU\n",
    "- weight normalization is applied to the convolutional filters\n",
    "- a spatial dropout after each dialted convolution for regularisation\n",
    "  - at each training step, a whole channel is zeroed out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "- Unlike RNN, convolutions can be done in parallel since the same filter is used in each layer\n",
    "- TCNs allow for better control of the model's memory size, and are easy to adapt to different domains.\n",
    "- TCNs avoids the problem of exploding/vanishing gradients\n",
    "- Since the filters are shared across a layer less memory is required during training\n",
    "- TCNs can handle sequential input data of arbitrary length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages:\n",
    "- TCNs requires more memory during evaluation because it takes in the raw sequence up to the effective history length\n",
    "- TCN may perform poorly for network trained in one domain and used in another domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Modeling Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors use a variety of sequence modeling tasks that are commonly used to benchmark the performance of recurrent architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sequence addition:__ a toy RNN problem used to stress test sequence models.\n",
    "\n",
    "\n",
    "<img src=\"figures/p3/problem-of-sequence-addition.png\" width=\"250\" />\n",
    "\n",
    "__Sequential MNIST__: frequently used to test a recurrent network’s ability to retain information from the distant past \n",
    "\n",
    "<img src=\"figures/p3/sequential-mnist.png\" width=\"400\" />\n",
    "\n",
    "__P-MNIST__:  more challenging, the order of the sequence is permuted at random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p3/results.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results strongly suggest that the generic TCN architecture with minimal tuning outperforms canonical recurrent architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word-level language modeling: \n",
    "- On the smaller PTB corpus, an optimized LSTM architecture outperforms the TCN\n",
    "- On the larger datasets (Wikitext-103 and LAMBADA), the TCN outperforms the LSTM results\n",
    "\n",
    "Character-level language modelling:\n",
    "- TCN outperforms the others\n",
    "- __Specialized architectures exist that outperform all of these!__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
