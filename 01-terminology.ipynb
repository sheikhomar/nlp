{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminology in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A corpus (plural corpora) is a representative collection of texts large enough to observe statistically meaningful frequencies of words and word sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding is the process of mapping words/phrases to vectors of real numbers. Sometimes, the term \"word embedding\" refers to a word vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An affinity matrix, also called a similarity matrix, contains distances between pairs of data points in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-Speech (POS) tagging also called grammatical tagging. Each word in the document is marked as e.g. noun, verb, adjective, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributional semantics is a theory about the semantics of a word. Essentially, it says that we can describe the meaning of words by understanding the context (neighbouring words) in which they appear. \"You shall know a word by the company it keeps.\" by J. R. Firth, 1957."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Language Model is a probabilistic model which predicts the probability that a sequence of tokens belongs to a language. The probabilities returned by a language model are mostly useful to compare the likelihood that different sentences are \"good sentences\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity vs L1/L2 Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L1 and L2 distance function quantify the amount of space \"we must travel\" to get between two given points. Another approach is to examine the angle between two vectors. When we map words/phrases to vectors of real numbers, we get high-dimensional dense vectors. Cosine similarity performs better when comparing whether two words are similar. Cosine has the advantage that it is a norm-invariant metric:\n",
    "\n",
    "$$\n",
    "cosine(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}\\cdot \\mathbf{v}}{\\lVert \\mathbf{u} \\lVert \\lVert \\mathbf{v} \\lVert}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A downstream task is a supervised learning task that utilise a pre-trained model or component. Examples of downstream tasks include machine translation, syntactic parsing, text classification, and machine comprehension, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF*IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short for Term Frequency â€“ Inverse Document Frequency. It is basically a product of two statistics; the term frequency (TF) and the inverse document frequecy (IDF). There are different ways to compute both statistics.\n",
    "\n",
    "The Term Frequency is the frequency of a word in a document i.e., the number of times a word $w$ appear in a document $d$.\n",
    "\n",
    "The Inverse Document Frequency (IDF) is a measure of how significant a word is or how much information the word provides in the whole corpus.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "W_{w,d} = \\text{tf}(w,d) \\log\\left[ \\frac{N}{D_w}  \\right]\n",
    "$$\n",
    "where\n",
    "- $W_{w,d}$ is the weight of the word $w$ in document $d$\n",
    "- $tf(w,d)$ is word/term frequency i.e., the number of times the word $w$ appears in document $d$\n",
    "- $N$ is the total number of documents in the corpus\n",
    "- $D_w$ is the number of documents containing the word $w$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight indicates how rare a word is. A high $W_{w,d}$ value indicates a rare word whereas a low value indicates a common term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) is a topic modelling method. Topic modelling is the process of identifying abstract topics that best describes a collection of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Termite: Visualization Techniques for Assessing Textual Topic Models: http://vis.stanford.edu/papers/termite\n",
    "\n",
    "<img src=\"figures/termite.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Retraining Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained word vectors can be trained further as part of the training the NLP system. However, word vector\n",
    "retraining should be considered for large training datasets so as to cover most words from the vocabulary. For small datasets, retraining word vectors will likely worsen performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
