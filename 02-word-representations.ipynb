{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language text can be seen as a sequence of words. One of the most fundamental tasks of NLP is to represent words. Ideally, we want to represent words such that similar words are \"closer\" to one another in some space than disimilar words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One naive representation is to let each word in the vocabolary sit in its own dimension. This is referred to as **one hot encoding**:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{a} = \\begin{bmatrix}1\\\\0\\\\\\vdots\\\\0\\end{bmatrix},\n",
    "\\mathbf{w}_{at} = \\begin{bmatrix}0\\\\1\\\\\\vdots\\\\0\\end{bmatrix}, \n",
    "\\cdots,\n",
    "\\mathbf{w}_{zebra} = \\begin{bmatrix}0\\\\0\\\\\\vdots\\\\1\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with this representation is that these vectors are sparse. Besides issues related to storage and computation of large sparse vectors, this representation is semantically weak because any vector do not relate to any other word vector. This representation does give any notion of relationships or similarity between words. Each word is treated as being orthogonal to all the other words so similarity using the dot product is zero for different words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better representation would express semantic similarity so that e.g. cats and kittens are close to each other. The theory of Distributional Semantics provides an idea of representing the meaning of a word by looking at its neighbouring words. This is based on the idea that words that have similar meanings appears in similar contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main approaches:\n",
    "- count-based methods\n",
    "  - Define a basis vocabulary $V$ of context words (exclude common function words like \"the\", \"a\", \"some\" and run stemming)\n",
    "  - Define a word window size $m$. This is used to define the context or neighbouring words of the word $\\mathbf{w}$. \n",
    "  - Count the basis vocabulary words occuring $m$ words to the left or right of each instance of a target word in the corpuse\n",
    "  - Form a vector representation of the target word based on these counts\n",
    "- direct prediction\n",
    "  - word2vec\n",
    "  - GloVe\n",
    "- task-based methods\n",
    "  - Train the word vectors at the same time as we learn the parameters of the network that uses them\n",
    "  - Idea is to learn a classifier based on features but also learn the features themselves in the process like Convolutional Neural Network that learn both features and classifiers at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a software package that consists of:\n",
    "- two algorithms: continuous bag-of-words (CBOW) and Skip Gram\n",
    "- two training methods; negative sampling and hierarchical softmax\n",
    "\n",
    "Word2vec attempts to create word vectors such that words with similar meanings or syntactical/grammatical function cluster together in an Euclidean space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting surrounding context words given a center word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main steps:\n",
    "- Using a window of size $m$ e.g. $m=1$, go through each word of the whole corpus\n",
    "- For each window, compute the probability of the context words given the center word. Essentially, we build a classifier for each window:\n",
    "$$\n",
    "p(w_{outside} | w_{center}) = \\frac{ \\exp( \\mathbf{u}_o^T \\mathbf{v}_c ) }{ \\sum_{w=1}^{V} (\\mathbf{u}_{w}^T \\mathbf{v}_c) }\n",
    "$$\n",
    "where $V$ is the size of the corpus, $\\mathbf{u}$ corresponds to the word embedding of the context/outside word and $\\mathbf{v}$ corresponds to the word embedding of the center word. The sum in the denominator involves the entire vocabulary.\n",
    "- Use the stochastic gradient descent to train the model. This means that we formulate an objective or a loss function for each classifier that maximises the probability function. Then we take the derivative of the objective function w.r.t. the vectors and set it to zero. Initially, the vectors $\\mathbf{u}$ and $\\mathbf{v}$ are random (sampled uniformly between two small numbers). By following the negative derivative iteratively, we find minimise the loss and increase the probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with the Skip Gram model is that in each window, there are only $2m+1$ words. This means that the derivative of the objective function is going to be very sparse.\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla \\mathcal{J} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "\\nabla_{v_{\\text{like}}} \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "\\nabla_{v_{\\text{nlp}}} \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{2dV}\n",
    "\\end{align}\n",
    "where $d$ is the number of dimensions in the embedding space e.g. $d=100$ and $V$ is the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick with the Skip Gram model is that we just train a small number of logistic regression for the true pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall objective function:\n",
    "\n",
    "$$\n",
    "\\mathcal{J}(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\mathcal{J}_t(\\theta)\n",
    "$$\n",
    "\n",
    "where $T$ is the total number of windows going through the corpus.\n",
    "\n",
    "The:\n",
    "$$\n",
    "\\mathcal{J}_t(\\theta) = -\\log \\sigma \\left( \\mathbf{u}_o^T \\mathbf{v}_c \\right)\n",
    "- \\sum_{i=1}^{k} \\mathbb{E}_{j \\sim P(w)} \\left[  \\log \\sigma \\left( - \\mathbf{u}_j^T \\mathbf{v}_c \\right)  \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term is the logarithm of the sigmoid function.\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-1}}\n",
    "$$\n",
    "\n",
    "Note:\n",
    "$$\n",
    "\\sigma(-x) = 1- \\sigma(x)\n",
    "$$\n",
    "\n",
    "The sigmoid function squishes any number in the range [0, 1]. In order words, we are trying to maximise the probability that the outside word and the center word co-occuring.\n",
    "\n",
    "<img src=\"figures/log-of-sigmoid.png\" width=\"600\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second term says that we randomly select $k$ negative samples/words from the corpus and for each these words, we minimise the probability of the negative word and the center word occuring together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling is based on unigram distribution:\n",
    "\n",
    "$$\n",
    "P(w) = \\frac{U(w)^{3/4}}{Z} \n",
    "$$\n",
    "The power makes less frequent words be sampled more often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main idea: Predict center word from sum of surrounding word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe: Global Vectors for Word Represention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe is method that attemts to combine the best of the count-based methods and the direct prediction methods.\n",
    "\n",
    "The count-based methods work by creating a co-occurrence matrix where we count how often words co-occur. Then we apply SVD on the co-occurence matrix.\n",
    "\n",
    "Advantages:\n",
    "- Relatively fast to train for smaller matrices\n",
    "- We only need the co-occurence counts (and not the entire corpus)\n",
    "\n",
    "Disadvantages:\n",
    "- The method captures mostly word similarities (word2vec model captures more pattern)\n",
    "- Disproprtionate importance are given to large counts\n",
    "\n",
    "The advantages and disadvantages of the direct prediction methods like the Skip Gram model:\n",
    "\n",
    "Advantages\n",
    "- Much better performance on downstream NLP tasks like entity recognition or part of speech tagging\n",
    "- Can capture complex patterns beyond word similarity\n",
    "\n",
    "Disadvantages:\n",
    "- Scale with corpus size, must go through every single window\n",
    "- Inefficient usage of statistics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{J}(\\theta) = \\frac{1}{2} \\sum_{i,j=1}^{W} f(P_{ij})(\\mathbf{u}_i^T \\mathbf{v}_j - \\log P_{ij}  )^2\n",
    "$$\n",
    "where\n",
    "- $P$ is the co-occurrence matrix \n",
    "- $P_{ij}$ is the number of times that the words $i$ and $i$ occur together.\n",
    "- $f$ is a way to cap the importance of very frequent words. It allows us to lower the weight of some frequent co-occurences e.g., the word \"the\" with other words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we want the value of $\\mathbf{u}^T\\mathbf{v}$ to be close to the overall count $\\log P_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the Skip Gram and GloVe:\n",
    "- Skip Gram model attempts to capture co-occurrences one window at a time.\n",
    "- GloVe model attempts to capture the counts of how often the words appear together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Traning GloVe model is faster\n",
    "- Scalable to large corpora\n",
    "- Good performance even with small corpus because of the efficient usage of the statistics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
