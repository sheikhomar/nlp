{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper: Convolutional Neural Networks for Sentence Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper proposes a simple CNN architecture that consists of one convolutional layer followed by a max-pooling layer for feature extraction and a fully-connected layer for classification. The input are word vectors trained on Google News articles or randomly initialised word vectors. Four variants of the CNN model are tested on 7 datasets. Results indicate that this simple CNN architecture delivers performance on par with more complex architectures like the Dynamic CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p2/cnn-architecture.png\" width=\"700\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As input, the network gets a sentence represented as a set of $N$ word vectors\n",
    "- A sentence can be seen as $N \\times K$ matrix where $N$ is the number of words in the sentence and $K$ is the number of dimensions in the word vectors\n",
    "- To produce one feature $c_i$\n",
    "  1. A filter $\\mathbf{w}\\in \\mathbb{R}^{K\\cdot H}$ is convolved to a window of $H$ words\n",
    "  2. The result of the convolution is added with a bias $b \\in \\mathbb{R}$\n",
    "  3. The sum is given to a non-linear function $f$ like the hyperbolic tangent\n",
    "    $$\n",
    "    c_i = f(\\mathbf{w} * \\mathbf{x}_{i:i+h-1} + b)\n",
    "    $$\n",
    "- The filter size $H$ determines the $n$-gram to look at e.g. if $H=2$ then we look at bigrams\n",
    "- The filter is applied by moving the filter with stride 1\n",
    "- The result of applying the filter on a sentence is a __feature map__:\n",
    "  $$\n",
    "  \\mathbf{c} = [c_1, c_2, c_3, \\cdots, c_{N-H+1}]\n",
    "  $$\n",
    "- Multiple filters are applied to each sentence to produce multiple feature maps\n",
    "- Max-over-time pooling operation is applied to each feature map: $\\hat{c} = \\max(\\mathbf{c})$\n",
    "- The __penultimate layer__ consists of $M$ max-pooled values:\n",
    "  $$\n",
    "  \\mathbf{z} = [\\hat{c}_1, \\hat{c}_2, \\cdots, \\hat{c}_M]\n",
    "  $$\n",
    "- Dropout is used on the penultimate layer\n",
    "- Additional regularisation: a constraint on $l_2$-norms of the weight vectors\n",
    "  - $\\lVert \\mathbf{w} \\rVert_2 = s$ whenever $\\lVert \\mathbf{w} \\rVert_2 > s$ after a gradient descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p2/table-2-performance.png\" width=\"600\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple CNN model delivers good performance\n",
    "- Performs as well as the more complex  Dynamic CNN model (Kalchbrenner et al., 2014)\n",
    "- Multichannel architecture needs more work on regularising the fine-tuning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discrepancy found in the CNN results in the Kalchbrenner paper\n",
    "- Dropout is a good regulariser\n",
    "- Random word vectors sampled from the uniform distribution $U(-a, a)$ gave slight better result where $a$ was chosen such that the randomly initialized vectors have the same variance as the pre-trained ones. \n",
    "- Adadelta gave similar results to Adagrad but required fewer epochs.\n",
    "- Word vectors produced by word2vec trained on Wikipedia gave better results than word vectors trained on Google News dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
