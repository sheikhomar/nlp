{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper: A Convolutional Neural Network for Modelling Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper focuses on the sentence modelling problem which is about representing a sentence's semantic content for the purposes of classification. The paper discussed other approaches for solving the sentence modelling problem:\n",
    "- Composition-based methods: vector representations of word meaning are used to create vectors for longer phrases.\n",
    "- Represent the meaning of sentences by automatically extracting logical forms (Zettlemoyer and Collins, 2005).\n",
    "- Neural sentence models based on convolution like Time-Delay Neural Networks (TDNN)\n",
    "\n",
    "The paper explains how the TDNN works, its advantanges and disadvantages. Then the authors introduce their invension Dynamic Convolutional Neural Network (DCNN). It is a new type of network that attempts to solve the shortcomings of TDNN while maintaining its advantages. This is done by using two operators:\n",
    "- Wide convolution is a type of convolution where the input is zero-padded with almost two times the size of filter\n",
    "- Dynamic $k$-max pooling finds $k$ largest values instead of a single maximum\n",
    "\n",
    "These two operators allow the network to:\n",
    "- discriminate whether a specific $n$-gram occurs in an input sentence.\n",
    "- tell the relative position of the most relavant $n$-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is tested on following tasks:\n",
    "- Predicting the sentiment (binary and multi-class) of movie reviews\n",
    "- Categorisation of questions in six question types in the TREC dataset\n",
    "- Predicting the sentiment of Twitter posts using distant supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional filter in the DCNN can be seen as a lingustic feature detector that learns during training to recognise a specific sequence of input words. These feature detectors can be visualised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## One-dimensional Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose \n",
    "- $\\mathbf{m} \\in \\mathbb{R}^M$ is a vector representing the weights of a convolutional filter of size $M$.\n",
    "- $\\mathbf{s} \\in \\mathbb{R}^S$ is a vector representing a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-dimensional convolution operation is defined as follows:\n",
    "\n",
    "$$\n",
    "c_j = \\mathbf{m}^T \\mathbf{s}_{j-m+1:j}\n",
    "$$\n",
    "\n",
    "where $c_j$ is the $j$th element of the vector $\\mathbf{c}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow vs Wide Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of the index $j$ gives rise to two types of convolutions:\n",
    "\n",
    "- **Narrow:** the value of $j$ can range from $M$ to $S$. This requires that the size of sentence vector is larger or equal to the size of the filter: $S > M$. The resulting sequence has $S-M+1$ elements. The figure above illustrates a narrow 1D convolution. In this example $S=9$, $M=3$ and $\\mathbf{c} \\in \\mathbb{R}^{7}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below illustrate a 1D convolution between a filter of size $M=3$ and a sentence vector of size $S=9$. The convolution yields the vector $\\mathbf{c}$ of size 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p1/1d-conv-narrow.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Wide:** the value of $j$ can range from $1$ to $S+M-1$. Out-of-range input values $s_i$ where $i < 1$ or $i > S$ are set to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p1/1d-conv-wide.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the result narrow convolution is a subsequence of the result of the wide convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the element $s_i \\in \\mathbb{R}$ of a sentence vector $\\mathbf{s}$ corresponds to the $i$th word in the sentence.\n",
    "\n",
    "Then the trained weights $\\mathbf{m}$ in a filter can be seen as a lingustic feature detector that can recognise a specific class of $n$-grams. For example, let us assume we have trained the filter $\\mathbf{m} = [0.2, 0.1, 0.4]^T$ to recognise the 3-gram *turtle loves shower*. This means that when the filter is convolved with a sentence with the phrase *turtle loves shower* then it will result in a high value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wide convolution have two major advantanges over narrow convolution:\n",
    "- Ensures that all weights in the filter reach the entire sentence, including the words at the margins. This is particularly significant when the filter size $M$ is relatively large such as $M=8$ or $M=10$. \n",
    "- Guarantees that the convolution of a filter $\\mathbf{m}$ to the input sentence $\\mathbf{s}$ always produces a valid non-empty result $\\mathbf{c}$, independently of the filter size $M$ and the sentence length $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Time-Delay Neural Network (Max-TDNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TDNN architecture was first proposed to classify <span title=\"one of the set of speech sounds in any given language that serve to distinguish one word from another\" style=\"border-bottom: 1px dotted #000;\">phonemes</span> in speech signals for automatic speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/p1/time-delay-nn.png\" width=\"477\" />\n",
    "\n",
    "Source: https://electroviees.wordpress.com/2013/08/06/speech-recoginition/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max-TDNN model:\n",
    "- Each sentence is represented as a $D \\times S$ matrix where column $i$ of the matrix represents the $i$th word $\\mathbf{w}_i \\in \\mathbb{R}^{D}$ in the sentence.\n",
    "- Sentence lengths may vary.\n",
    "- Each filter is of size $D \\times M$. We can see this a $D$ one-dimensional filters for size $M$.\n",
    "- Each row of the filter is the convolved with the corresponding row of the input.\n",
    "- The convolution (narrow) yields a matrix.\n",
    "- Max pooling addresses the problem of varying sentence lengths. The Max-TDNN takes the maximum of each row in the resulting matrix yielding a vector of $D$ values, $\\mathbf{c}_{max}$.\n",
    "- The fixed-sized vector $\\mathbf{c}_{max}$ is then used as input to a fully connected layer for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p1/max-tdnn-example.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of Max-TDNN:\n",
    "- It is sensitive to the order of the words in the sentence\n",
    "- It does not depend on external language-specific features such as dependency or constituency parse trees.\n",
    "- It also gives largely uniform importance to the signal coming from each of the words in the sentence, with the exception of words at the margins that are considered fewer times in the computation of the narrow convolution. \n",
    "\n",
    "Limitations of Max-TDNN:\n",
    "- Higher-order and long-range feature detectors cannot be easily incorporated into the model. The range of the feature detectors is limited to the span $M$ of the weights. Increasing $M$ or stacking multiple convolutional layers of the narrow type makes the range of the feature detectors larger; at the same time it also exacerbates the neglect of the margins of the sentence and increases the minimum size $S$ of the input sentence required by the convolution.\n",
    "- Max pooling cannot distinguish whether a relevant feature in one of the rows occurs just one or multiple times and it forgets the order in which the features occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Convolutional Neural Network (DCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCNN attempts to solve the limitations of Max-TDNN while preserving the advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p1/dcnn-architecture-example.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCNN uses wide one-dimensional convolutions in every layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic $k$-Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCNN  introduces dynamic $k$-max pooling operator:\n",
    "- Given a sequence of values $\\mathbf{p} \\in \\mathbb{R}^{P}$, $k$-max pooling operator returns the subsequence of $k$ largest values in that sequence.\n",
    "- The order of the values corresponds to their original order in $\\mathbf{p}$\n",
    "- It is a generalisation of the max pooling operator which only return the largest value\n",
    "- The pooling parameter $k$ can be dynamically altered by making $k$ a function of other aspects of the network or the input. One example is \n",
    "$$\n",
    "k_l = \\max\\left( k_{top}, \\left \\lceil \\frac{L-l}{L}S \\right \\rceil \\right)\n",
    "$$\n",
    "where \n",
    "   - $l$ is the number of the current convolutional layer for which the pooling is applied, \n",
    "   - $L$ is the total number of convolutional layers, \n",
    "   - $k_{top}$ is a fixed pooling parameter for the topmost convolutional layer.\n",
    "   - $S$ is the length of an input sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p1/k-max-pooling-example.png\" width=\"900\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why?\n",
    "- Allows us to extract the $k$ most active features in a sequence.\n",
    "- It preserves the order of the active features, but is insensitive to their specific positions.\n",
    "- It can also discern more finely the number of times the feature is highly activated in $\\mathbf{p}$ and the progression by which the high activations of the feature change across $\\mathbf{p}$\n",
    "- The dynamic pooling parameter $k$ allows for a smooth extraction of higher-order and longer-range features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear Feature Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After $k$-max pooling is applied to the result of a convolution, a bias $\\mathbf{b} \\in \\mathbb{R}^D$ and a nonlinear function $g(\\cdot)$ are applied component-wise to the pooled matrix.\n",
    "\n",
    "There is a single bias value for each row of the pooled matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of Feature Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After applying the convolution, max-pooling and a non-linear function, we get a **feature map**. \n",
    "- A feature map produced in the $i$-th convolutional layer is called an $i$-th order feature map and denoted as $\\mathbf{F}^{i}$.\n",
    "- Each filter (a $D \\times M$ matrix) produces a feature map.\n",
    "- Each convolutional layer can have $n$ filters and thus produce $n$ feature maps; $\\mathbf{F}^{i}_1,\\mathbf{F}^{i}_2,\\cdots,\\mathbf{F}^{i}_n$.\n",
    "- A second-order feature map $\\mathbf{F}^{2}$ can be computed by convolving a filter represented by $\\mathbf{m}^{2}_k$ with each feature map in the preceeding layer $\\mathbf{F}^{1}_k$ and summing the results:\n",
    "$$\n",
    "\\mathbf{F}^{2} = \\sum_{k=1}^{n} \\mathbf{m}^{2}_k * \\mathbf{F}^{1}_k\n",
    "$$\n",
    "where \n",
    "  - $n$ is the number of feature maps from the preceeding layer\n",
    "  - $*$ is the wide convolution\n",
    "  - $\\mathbf{m}^{2}_k$ is order-4 tensor\n",
    "- In general, each feature map $\\mathbf{F}^{i}_j$ can be computed:\n",
    "$$\n",
    "\\mathbf{F}^{i}_j = \\sum_{k=1}^{n} \\mathbf{m}^{2}_{j,k} * \\mathbf{F}^{i-1}_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After a convolution and before max pooling, one just sums every two rows in a feature map component-wise.\n",
    "- For a feature map of $D$ rows, folding returns a feature map of $D/2$ rows, thus halving the size of the representation. \n",
    "- With a folding layer, a feature detector of the i-th order depends now on two rows of feature values in the lower maps of order $i - 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p1/folding-example.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of the DCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can discriminate whether a specific $n$-gram occurs in an input sentence. \n",
    "  - The filters of the wide convolution in the first layer can learn to recognise specific $n$-grams ( where $n \\leq M$ and $M$ is the size of the filter)\n",
    "  - The filters in the first layer is set to a large value of $M$ e.g. $M=10$\n",
    "- Can tell the relative position of the most relavant $n$-grams\n",
    "  - The $k$-max pooling operation maintains the order and relative position of the $n$-grams recognised by the convolution layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The convolution and pooling layers induce an internal feature graph over the input.\n",
    "  - Nodes that are not selected by the pooling operation at a layer are dropped from the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p1/induced-feature-graph.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1: Subgraph of a feature graph induced\n",
    "over an input sentence in a Dynamic Convolutional Neural Network. The full induced graph\n",
    "has multiple subgraphs of this kind with a distinct\n",
    "set of edges; subgraphs may merge at different\n",
    "layers. The left diagram emphasises the pooled\n",
    "nodes. The width of the convolutional filters is 3\n",
    "and 2 respectively. With dynamic pooling, a filter with small width at the higher layers can relate\n",
    "phrases far apart in the input sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested the network on three different experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** The prediction of the sentiment of movie reviews in the Stanford Sentiment Treebank\n",
    "\n",
    "\n",
    "<img src=\"figures/p1/experiment-1-results.png\" width=\"300\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** The classification of questions to one of six question types (the TREC questions dataset):\n",
    "\n",
    "\n",
    "<img src=\"figures/p1/experiment-2-results.png\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Prediction of sentiment of tweets with distant supervision. The training set is labelled automatically according to the emoticon that occurs in them but the test set consists of about 400 hand-annotated tweets.\n",
    "\n",
    "\n",
    "<img src=\"figures/p1/experiment-3-results.png\" width=\"200\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Feature Detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A filter in the DCNN can be seen as a lingustic feature detector that learns during training to recognise a specific sequence of input words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first layer, the sequence is an $n$-gram from the input sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In higher layers, sequences can be made of multiple separate important $n$-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below illustrates the two feature detectors (positive or negative sentiment) in the first layer of a DCNN trained on the binary sentiment task (Experiment 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p1/top-five-7-grams-part-1.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detectors for particles such as \"not\" that negate sentiment and \"too\" that potentiate sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p1/top-five-7-grams-part-2.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They also found detectors for multiple other notable constructs:\n",
    " - all\n",
    " - or\n",
    " - with ... that\n",
    " - as ... as\n",
    "\n",
    "The feature detectors learn to recognise not just single $n$-grams, but patterns within $n$-grams that have syntactic, semantic or structural significance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
