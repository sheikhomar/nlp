{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper: Character-level Convolutional Networks for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper proposes character-based convolution networks as a method for text classification. \n",
    "\n",
    "Like the traditional ConvNets for images, the character-based ConvNet consists of convolutional layers followed by a pooling layers. In the convolution layer, the input matrix is convolved with multiple one-dimensional filters and summed to form single vectors. The pooling layer performs non-overlapping max-pooling. Before the text documents are fed to the ConvNet, they are quantised. This means that each letter is converted to a one-hot vector and concatenated together into a matrix. \n",
    "\n",
    "This new method is compared to different traditional methods and deep learning methods (two word-based ConvNets and an LSTM) using several large datasets. The authors conclude that although the character-based ConvNet is an effective method, it is not a silver bullet. The performance depends on factors like the size of the dataset, whether the documents are curated and the choice of the alphabet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define an alphabet of letters\n",
    "  - In the article, the alphabet consists of 26 english letters, 10 digits, 33 other characters and the new line character so 70 characters in total.\n",
    "2. Quantise each character as one-hot vector \n",
    "  - Each character vector has 70 dimensions.\n",
    "  - Spaces and characters outside the alphabet are zero vectors.\n",
    "  - Character quantization order is backward??\n",
    "3. Quantise each document by concatinating the corresponding char vectors.\n",
    "  - Document size is fixed to $l_0$ e.g. $l_0 = 1014$\n",
    "  - Documents with more than $l_0$ characters are truncated.\n",
    "  - Documents with less than $l_0$ characters are padded with zero vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p0/quantisation-example.png\" width=\"300\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Convolutional Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p0/convolutional-module.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(g, f, d=1):\n",
    "    \"\"\"\n",
    "    Computes 1D convolution\n",
    "    \n",
    "    g(x) is the input array\n",
    "    f(x) is the 1D filter\n",
    "    d is the stride\n",
    "    \"\"\"\n",
    "    l = g.shape[0] # Size of the input array\n",
    "    k = f.shape[0] # Size of the filter\n",
    "    c = k - d + 1  # Offset constant\n",
    "\n",
    "    output_size = int(np.floor((l - k + 1)/d))\n",
    "    h = np.zeros(output_size)\n",
    "    \n",
    "    # Naive implementation\n",
    "    for y in range(output_size):\n",
    "        for x in range(k):\n",
    "            # Compensate for zero indexing\n",
    "            gi = (y+1) * d - x + c - 2\n",
    "            h[y] += f[x] * g[gi]\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = np.array([1, 0, 0, 1, 1, 1, 0, 0, 1, 1])\n",
    "f = np.array([0, 0, 1])\n",
    "h = convolve(g, f)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = np.array([1, 0, 0, 1, 1, 1, 0, 0])\n",
    "(h == expected).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the convolution operation is performed, the result is then summed column-wise. Here is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p0/1d-convolutional-example.png\" width=\"900\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Max-Pooling Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p0/max-pool-module.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxPool(g, k, d=1):\n",
    "    l = g.shape[0] # Size of the input array\n",
    "    c = k - d + 1  # Offset constant\n",
    "    \n",
    "    # Note: using ceil() even though the paper uses floor().\n",
    "    # If floor() is used then the output size is incorrect\n",
    "    output_size = int(np.ceil((l - k + 1)/d))\n",
    "    h = -np.inf * np.ones(output_size)\n",
    "    \n",
    "    for y in range(output_size):\n",
    "        for x in range(k):\n",
    "            # Compensate for zero indexing\n",
    "            gi = (y+1) * d - x + c - 2\n",
    "            if g[gi] > h[y]:\n",
    "                h[y] = g[gi]\n",
    "            \n",
    "    return h\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  5.,  6., 11., 11., 11., 42.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = np.array([3, 4, 3, 5, 6, 11, 4, 3, 42])\n",
    "h = maxPool(g, k=3)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = np.array([ 4, 5, 6, 11, 11, 11, 42])\n",
    "(h == expected).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, the authors mention that pooling layers are non-overlapping i.e., the size of the pooling and the stride is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4., 11., 42.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = np.array([3, 4, 3, 5, 6, 11, 4, 3, 42])\n",
    "h = maxPool(g, k=3, d=3)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = np.array([ 4, 11, 42])\n",
    "(h == expected).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the max pooling operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p0/max-pooling-example.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors tested two character-based ConvNet configurations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p0/network-configurations.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 from the paper:\n",
    "\n",
    "<img src=\"figures/p0/figure-1.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Network weights are initialised using a Gaussian distribution.\n",
    "  - The mean and standard deviation used for initializing the large model is (0; 0.02) and small model (0; 0.05).\n",
    "- Network is trained with Stochastic Gradient Descent with momentum 0.9, minibatch size 128\n",
    "- Initial step size 0.01, halved every 3 epoches for 10 times\n",
    "- Each epoch takes a fixed number of random training samples uniformly sampled across classes.\n",
    "- Data augmented by replacing words or phrases with their synonyms\n",
    "  - Used English thesaurus by WordNet\n",
    "  - Every synonym to a word is ranked by the semantic closeness to the most frequently seen meaning\n",
    "  - Randomly replaced $r$ number of words in a document. The synonym is also chosen randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multinomial logistic regression using hand-crafted features:\n",
    "  - Bag-of-words and its TFIDF\n",
    "  - Bag-of-ngrams and its TFIDF\n",
    "  - Bag-of-means on word embedding\n",
    "- Word-based ConvNets\n",
    "  - using pretrained word2vec embedding\n",
    "  - using lookup tables\n",
    "- Long-short term memory using pretrained word2vec embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p0/datasets.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 4 shows testing errors of all the models. Numbers are in percentage. \n",
    "\n",
    "Following abbreviations are used:\n",
    "- \"Lg\" stands for \"large\"\n",
    "- \"Sm\" stands for \"small\"\n",
    "- \"w2v\" is an abbreviation for \"word2vec\"\n",
    "- \"Lk\" stands for \"lookup table\"\n",
    "- \"Th\" stands for thesaurus. \n",
    "- ConvNets labeled \"Full\" are those that distinguish between lower and upper letters. Observed that it usually (but not always) gives worse results when such distinction is made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/p0/experiment-results.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Character-level ConvNet is an effective method\n",
    "- ConvNets do better when the dataset goes to the scale of several millions\n",
    "- ConvNets may work well for user-generated data\n",
    "- Distinguishing between uppercase and lowercase letters could make a difference. For million-scale datasets, it seems that not making such distinction usually works better.\n",
    "- Semantics of tasks may not matter\n",
    "- Bag-of-means is a misuse of word2vec\n",
    "- There is not a single machine learning model that can work for all kinds of datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
