{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency parsing is the task of analyzing the structure of sentences in (human) languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why study the structure of sentences? Because we want to be able to interpret language correctly. Humans communicate complex ideas by composing words together into bigger units to convey complex meanings. We need to know what is connected to what."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In question answering tasks, we decompose a question into its syntactical parts like subject and objects in order to figure out what the user is asking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two key tools to solve this problem:\n",
    "- Constituency Grammar\n",
    "- Dependency Structure/Grammar (very popular today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency Structure/Grammar:\n",
    "- The idea of dependency grammar goes back to Panini's grammar 5th century BCE. \n",
    "- It is the basic approach of first millennium Arabic grammarians.\n",
    "- Modern dependency work often sourced to L. Tesnière (1959)\n",
    "  - Was dominant approach in the East in 20th Century (Russia, China, ...). \n",
    "  - Dependency grammars work well for describing \"free-er\" word order languages. English is fixed word order language. \n",
    "- David Hays, one of the founders of U.S. computational linguistics, built early (first?) dependency parser (Hays 1962)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constituency/Context-Free Grammar:\n",
    "- New invention\n",
    "- Invented in the 20th century by R.S. Wells in 1947. Then by Chomsky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constituency Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constituency grammar is a way to describe the syntax of a language or the structure of sentences. The idea is to use phrase structure grammers (or context-free grammers) to organize words into nested constituents. A grammer is a set of rules that allows us to generate valid sentences of a language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/chomsky.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other tool, Dependency Structure, describes which words depend on which other words. When one word $w_1$ is dependent on another word $w_1$, then $w_1$ modifies or is an argument of $w_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a sentence:\n",
    "\n",
    "\"Look for the large barking dog by the door in a crate.\"\n",
    "\n",
    "- The word *barking* is dependent on *dog* because *barking* modifies *dog*. \n",
    "- The word *large* is dependent on *dog* since it modifies *dog*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we indicate dependencies by arrows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/dependency-structure-example.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguous Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ambiguous sentence can be thought about in terms of different dependencies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/dependency-structure-ambiguity.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universal Dependencies treebank is a database of sentences where people have manually annotated sentences into dependency graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/ud-treebank-example.png\" width=\"700\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Source: [SETS treebank](http://bionlp-www.utu.fi/dep_search/) search maintained by the University of Turku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a treebank seems a lot slower and less useful than building a grammar that can generate infinite number of sentences.\n",
    "\n",
    "A treebank has many advantages:\n",
    "- Reusability of the labor (grammars cannot be reused because everyone makes up their own)\n",
    "  - Many parsers, part-of-speech taggers, etc. can be built on it\n",
    "  - They are valuable resource for linguistics because they give examples of how a language is spoken and syntactic analysis of sentences\n",
    "- Broad coverage, not just a few intuitions\n",
    "- Frequencies and distributional information\n",
    "  - E.g. co-occurence of words\n",
    "- More crucially, it provides a way to evaluate systems because it provides ground-truth gold standard data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theory of dependency syntax postulates that syntactic structure consists of relations between lexical items (i.e., words), normally binary asymmetric relations (i.e., we draw arrows) called dependencies.\n",
    "\n",
    "The arrows/dependencies are commonly typed with the name of grammatical relations such as subject, prepositional object, apposition, etc. This is referred to as **typed dependency grammar**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/dependency-structure-example-2.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrow connects a **head** (or governor/superior/regent) with a **dependent** (or modifier/inferior/subordinate). But there is no consensus of how to draw the arrow. Tesnière had the arrow point from head to dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, dependencies form a tree which means that they have certain graph theorical properties:\n",
    "- they have a single head (root node). \n",
    "- connected\n",
    "- they are acyclic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we add a psuodo word ROOT at the start of a sentence so every word is a dependent of precisely one other node. This makes the formalism (math) easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Conditioning Preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What information can help up decide which words are dependent on which other words. We can gather information from various sources:\n",
    "1. Bilexical affinitiees\n",
    "   - Discussion of issues is plausible\n",
    "2. Dependency distance\n",
    "   - Although there may be some long distance between depency, most of the time there is dependency with nearby words\n",
    "3. Intervening material\n",
    "   - Dependencies rarely span intervening verbs or punctuation\n",
    "4. Valency of heads\n",
    "   - How many dependents on which side are usual for a head?\n",
    "   - How likely is a head to have dependents in what number and what size?\n",
    "   - For example, a word like \"the\" is not likely to have any dependent at all anywhere.\n",
    "   - Verbs tend to have many dependents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projectivity is defined as \"There are no crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies parallel to a CFG tree must be projective. Forming dependencies by taking 1 child of each category as head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure belows illustrates a **non-projective** dependency because two of arcs are crossing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/non-projectivity.png\" width=\"500\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency theory normally allows non-projective structures to account for displaced constituents. You can't easily get the semantics of certain constructions right without these nonprojective dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods of Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many method for dependency parsing:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dynamic programming: Eisner (1996) gives a clever algorithm with complexity $\\mathcal{O}(n^3)$, by producing parse items with heads at the ends rather than in the middle. This is normally used for context-free grammars.\n",
    "\n",
    "2. Graph algorithms: You create a Minimum Spanning Tree (MST) for a sentence. McDonald et al.'s (2005) MSTParser scores dependencies independently using an ML classifier. They use MIRA, for online learning, but it can be something else.\n",
    "\n",
    "3. Constraint Satisfaction: We can view dependecy parsing as constraint satisfaction problem. Edges that don't satisfy hard constraints are eliminated. Karlsson (1990), etc.\n",
    "\n",
    "4. **Transition-based parsing** called \"deterministic dependency parsing\" when it was first introduced. The idea is to greedily choose attachments guided by good machine learning classifiers. MaltParser (Nivre et al. 2008). Has proven highly effective. This method is the dominant way of performing dependency parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition-based Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transition scheme describes the rules for parsing. It provides a set of legal actions that the parser can take to parse a sentence. One such transition scheme is the arc-standard scheme:\n",
    "\n",
    "<img src=\"figures/transition-scheme.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser works like a shift-reduce parser.\n",
    "\n",
    "The dependency parser performs a sequence of bottom-up actions, roughly like \"shift\" or \"reduce\" in a shift-reduce parser. The \"reduce\" actions are specialized to create dependencies with head on the left or on the right.\n",
    "\n",
    "The parser has:\n",
    "- a stack $\\sigma$ \n",
    "- a buffer $\\beta$\n",
    "- a set of dependency arcs $A$ where attachment decisions are recorded\n",
    "\n",
    "It can perform three operations:\n",
    "- Shift\n",
    "- Left arc\n",
    "- Right arc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shift**: take the word from the top of the buffer and put it on the top of the stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/shift-operation-example.png\" width=\"200\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Left arc:** make attachment decision by adding a word as a dependent to the left\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"I\" is dependent on \"ate\". Therefore, the left-arc operation removes \"I\" from the stack and records the attachment decision:\n",
    "\n",
    "```\n",
    "A += nsubj(ate -> I)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/left-arc-operation-example.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Right arc:** make attachment decision by adding a word as a dependent to the right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word on top of the stack is made dependent on the second-top word of the stack. \n",
    "\n",
    "For example, we remove the word \"ate\" from the stack and record that \"fish\" is depedent on \"ate\" i.e., we add an arc/arrow in our dependency tree.\n",
    "\n",
    "Similarly, \"ate\" is dependent on \"[root]\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/right-arc-operation-example.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finish the parsing when we have zero elements in the stack and one element in the buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining question is how do we choose which operation/action to perform next at any point in time?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nivre and Hall (2005) proposed the following in the MaltParser:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaltParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each action is predicted by a discriminative classifier (e.g. SVM or softmax classifier) over each legal move. This is possible because we have treebanks of sentences. For each tree structure, there is a unique sequence of shifts, left-arcs and right-arcs that give us the right structure. Given a tree structure, the classifier tell us which operation/action to perform next. There are maximum three choices if the operations are untyped i.e., if we don't classify the grammatically meaning of the word. With typed, we have $|R| \\times 2 + 1$ choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest form, there is NO search. But you can profitably do a beam search if you wish (slower but better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's accuracy is fractionally below the state of the art in dependency parsing, but it provides very fast linear time parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional Feature Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As input for the classifier, we can use a combination of features including:\n",
    "- the word on the top of the stack\n",
    "- the part-of-speech of that word\n",
    "- the first word in the buffer\n",
    "- the part-of-speech of the word in the buffer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent a configuration of stack, buffer and part-of-speech of words as a large binary vector with approx. 10-100 mio. dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/conventional-feature-representation.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also represent features that are conjunctions of attributes called indicator features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/indicator-features.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problem #1: sparse\n",
    "- Problem #2: incomplete because many configurations will never have been seen by the classifier\n",
    "- Problem #3: expensive computation. It turns out the symbolic dependency parsers are slow because more than 95% of parsing time is consumed by feature computation and looking up weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better alternative is to learn a dense and compact feature representation (approx. 1000 dimensions) using a dependency parser based on neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Neural Dependency Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are extracted based on the stack/buffer positions just like the MaltParser. \n",
    "\n",
    "Each word is represented as a $d$-dimensional dense vector (i.e., word embedding). Similar words are expected to have close vectors.\n",
    "\n",
    "Meanwhile, part-of-speech tags (POS) and dependency labels are also represented as $d$-dimensional vectors. The smaller discrete sets also exhibit many semantical similarities.\n",
    "- NNS (plural noun) should be close to NN (singular noun)\n",
    "- NUM (numerical modifier) should be close to AMOD (adjective modifier)\n",
    "\n",
    "The dense representation is found by looking the tokens up in the embedding matrix and concatenating them into one long vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/dense-representation.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"figures/network-architecture.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/further-work.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Graph-based Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSTParser and TurboParser are graph-based parsers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute a score for every possible dependency for each edge\n",
    "- Then add an edge from each word to its highest-scoring candidate head\n",
    "- And repeat the same process for each other word\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/neural-graph-based-parser.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the result of a dependency parser is straightforward:\n",
    "- Perform the parsing\n",
    "- Compare it with the ground-truth from the treebank\n",
    "- The accuracy is the number of dependencies that are correctly parsed divided by the total number of dependencies.\n",
    "- Unlabelled Accuracy Score (UAS) is measure where we look at the arrows and ignore the labels.\n",
    "- Labelled Accuracy Score (LAB) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/evaluation-dependency-parsing.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
